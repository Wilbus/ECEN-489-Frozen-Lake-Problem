{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.188\n",
      "Q values:\n",
      "[[7.51849914e-02 8.55309567e-02 7.29175972e-02 6.08190527e-02]\n",
      " [5.24238749e-03 6.64881123e-03 5.45620466e-03 7.62822870e-02]\n",
      " [2.30090697e-02 2.83609885e-02 1.78414488e-02 3.01659243e-02]\n",
      " [2.60800604e-02 4.31970116e-03 1.59183456e-04 2.81267317e-02]\n",
      " [8.63909242e-02 8.24148377e-02 8.36517150e-02 7.88204270e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.71601656e-02 7.47993014e-04 1.17160836e-03 7.31297056e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.75367505e-02 2.58660951e-02 6.66402965e-04 8.25057852e-02]\n",
      " [1.40005758e-02 5.43890436e-02 4.63624757e-04 1.16138576e-02]\n",
      " [3.00160151e-02 2.45150652e-02 1.42592386e-03 8.87799878e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.27555985e-02 6.85753612e-02 5.05729633e-01 1.52022604e-01]\n",
      " [2.88598198e-01 9.78284066e-01 2.75161712e-01 3.10382457e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "We only print the last state in each episode, to see if our agent has reached the destination or fallen into a hole\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 33\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Number of steps 99\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 29\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 29\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 26\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 99\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 55\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 65\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 6\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Q2. In some episodes above, the policy isn't reaching the goal, why?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "# Feel free to play with these hyperparameters\n",
    "\n",
    "total_episodes = 1000         # Total episodes\n",
    "test_episodes = 10            # Test episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 100               # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n",
    "\n",
    "# Initializations\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose an action a in the current state (greedy or explore)\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)  \n",
    "        # exploitation (taking the max Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            # Enter code here\n",
    "            ## Hint: Greedily choose an action according to Q value\n",
    "            \n",
    "            action = np.argmax(qtable[state,:])\n",
    "            \n",
    "            ###debugging info###\n",
    "            #print(\"Greedy action: \", end=\"\")\n",
    "            #print(action)\n",
    "            ####################\n",
    "            \n",
    "        # exploration\n",
    "        else:\n",
    "            # Enter code here\n",
    "            ## Hint: Randomly choose an action\n",
    "            \n",
    "            action = random.randint(0, action_size - 1)\n",
    "            \n",
    "            ####debugging info###\n",
    "            #print(\"Random action: \", end=\"\")\n",
    "            #print(action)\n",
    "            #####################\n",
    "            \n",
    "        # Take this action and observe\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Do a Q update\n",
    "        # Enter code here\n",
    "        ## Hint: One line update equation convert to one line code, start with \"qtable[state, action] = ...\"\n",
    "        \n",
    "        sample = reward + gamma * np.amax(qtable[new_state,:])\n",
    "        qtable[state][action] = (1 - learning_rate) * qtable[state][action] + learning_rate * sample\n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Decay epsilon to reduce exploration as time progresses\n",
    "    # Enter code here to assign a decay value to \"decay_parameter\"\n",
    "    \n",
    "    ## Hint: \n",
    "    ## 1. Use inbuilt polynomial, exponential(, or whatever works) functions to decay epsilon\n",
    "    ## 2. \"decay_parameter\" is a function of \"decay_rate\" and \"episode\"\n",
    "    \n",
    "    decay_parameter = np.exp(-decay_rate*episode)  \n",
    "    #print(decay_parameter)\n",
    "    \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*decay_parameter\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print(\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(\"Q values:\")\n",
    "print(qtable)\n",
    "\n",
    "'''Q1. In short, explain why fixed \"epsilon\" above isn't the best choice? \n",
    "(Hint: You can keep epsilon fixed and see whether your reasoning explains the behavior)\n",
    "A fixed epsilon means that the agent will always choose to explore (choose a random action). This may not be the best\n",
    "step to take if the agent has already explored enough of the world and has a policy that is good enough to follow. Hence,\n",
    "a decreasing epsilon is good because the agent can explore the world and then act on which actions seem to be\n",
    "currently the best(exploitation). '''\n",
    "\n",
    "########################################################################\n",
    "#################### Final policy animation ############################\n",
    "########################################################################\n",
    "\n",
    "print(\"We only print the last state in each episode, to see if our agent has reached the destination or fallen into a hole\")\n",
    "env.reset()\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Taking action with Q learning\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            \n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "\n",
    "'''Q2. In some episodes above, the policy isn't reaching the goal, why?\n",
    "There is still a probabilty that the agent will not take the action according to the optimal policy.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
